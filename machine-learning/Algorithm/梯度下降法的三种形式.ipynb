{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降的解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在微积分中对多元函数的参数求偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度，比如函数$f(x,y)$分别对x/yqiu piandaoshu \n",
    "求得的梯度向量就是$(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y}）^T$。\n",
    "函数沿着梯度的方向，增加最快（上升），函数沿着负梯度方向，减小最快（下降）。\n",
    "\n",
    "引用吴恩达教授上的比喻，比如我们站在一座大山某个位置，如果我们只迈一小步，往哪个方向走能使我下山的速度最快，梯度下降就是这样工作的， 我向下降最快的方向走一步，事实上这正是梯度的方向,之后再走一步，然后站在了一个新的位置，然后再计算一次梯度，再向这个梯度的方向走一步，重复这样一直走..直到走到最底下，在函数中就是得到一个局部最小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降的性质\n",
    "梯度下降依赖于参数初始值，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习的过程中，通常会用到梯度下降法来进行训练，梯度下降法包含三种形式，它们分别是批量梯度下降（$BGD$）,随机梯度下降法（$SGD$）,\n",
    "以及小批量梯度下降（$MBGD$）,下面以线性回归算法来对三种算法进行比较。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设dataset中有 $m$ 个样本，每个样本有 $n$ 个特征,其中 $i$ 表示第$i$个样本,$j$ 表示样本中第 $j$ 个特征\n",
    "$\\theta$为（n,1）$x$为（1，n）,m个样本，$y$为（m,1）\n",
    "\n",
    "一般线性回归函数的假设函数（hypothesis function）:\n",
    "    $$\n",
    "    h_{\\theta}(x)= \\theta_{0} + \\theta_{1}x_{1} +\\cdots + \\theta_{n}x_{n}\n",
    "    =\\sum_{j=0}^n \\theta_{j}x_{j} = \\theta^Tx\n",
    "    $$\n",
    "\n",
    "最终要求计算出 $\\theta$的值，并选择最优的$\\theta$值构成算法公式\n",
    "\n",
    "对应的损失函数为：\n",
    "\n",
    "$$\n",
    "loss(y_{j},\\hat y_{j})=J(\\theta)=\\frac{1}{2} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降的求解过程：\n",
    "\n",
    "首先初始化 $\\theta = \\overrightarrow {0}$\n",
    "\n",
    "$\\alpha$：学习率、步长(取得过大可能会导致错过最优解，取得太小收敛的很慢)\n",
    "\n",
    "持续更新 $\\theta$ ,来减少 $J(\\theta)$ 沿着负梯度方向迭代(:)，更新后的$\\theta$使$J\\left(\\theta\\right)$更小: $$\\theta_{j} : = \\theta_{j}- \\alpha * \\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta)=\\frac{1}{2} \\sum_{i=1}^m  \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right)^2$$\n",
    "下面介绍只有一组训练数据，对目标函数求偏导的过程如下：\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}=\\frac{\\partial }{\\partial \\theta_{j}}\\frac{1}{2} \\left( h_{\\theta}(x) - y\\right)^2 \\\\\n",
    "= \\left( h_{\\theta}(x) - y\\right).\\frac{\\partial}{\\partial \\theta_{j}}\\left( h_{\\theta}(x) - y\\right) \\\\\n",
    "= \\left( h_{\\theta}(x) - y\\right).\\frac{\\partial}{\\partial \\theta_{j}}\\left( \\theta_{0}x_{0}+\\theta_{1}x_{1}+\\theta_{j}x_{j}+\\cdots + \\theta_{n}x_{n} - y\\right) \\\\\n",
    "= \\left( h_{\\theta}(x) - y\\right).\\frac{\\partial}{\\partial \\theta_{j}}\\left(\\theta_{j} x_{j}\\right)\\\\\n",
    "= \\left( h_{\\theta}(x) - y\\right)x_{j}\n",
    "$$\n",
    "\n",
    "所以梯度下降更新为：\n",
    "   $$\\theta_{j} : = \\theta_{j}- \\alpha * \\left( h_{\\theta}(x) - y\\right)x_{j}$$\n",
    "   这样就求出了第$j$个特征参数，如果想把所有特征的参数求出来呢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   $$\\theta : = \\theta- \\alpha * \\left( h_{\\theta}(x) - y\\right)x$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$(h_{\\theta}(x)-y)$ shape是(1)，$\\theta$ shape是（n,1),$x$ shape是（1,n），所以在实际运算过程中x要进行转置，这样每个特征对应的参数就整个求出来变成(n,1)的向量\n",
    "$$\\theta : = \\theta- \\alpha * \\left ({x}^T)( h_{\\theta}(x) - y\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量梯度下降法$BGD$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量梯度下降（ Batch  Gradient Descent ,简称$GBD$），batch意味着梯度下降的每一次迭代都需要遍历整个训练集，因为需要基于$m$个样本进行求和：\n",
    "\n",
    "$$\n",
    "\\theta_{j}: = \\theta_{j} - \\alpha\\sum_{i=1}^m \\frac{\\partial}{\\partial \\theta_{j}}= \\theta_{j} - \\alpha\\sum_{i=1}^m\\left(y^{(i)} -  h_{\\theta}(x^{(i)})\\right)x_{j}^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x1  x2\n",
      "0   1   1\n",
      "1   2   2\n",
      "2   3   1\n",
      "3   4   2\n",
      "4   5   1\n",
      "5   6   2\n",
      "    0  1  2  3  4  5\n",
      "x1  1  2  3  4  5  6\n",
      "x2  1  2  1  2  1  2\n",
      "[[2]\n",
      " [4]\n",
      " [4]\n",
      " [6]\n",
      " [6]\n",
      " [8]]\n",
      "[[0]\n",
      " [0]]\n",
      "   0\n",
      "0  0\n",
      "1  0\n",
      "2  0\n",
      "3  0\n",
      "4  0\n",
      "5  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#定义一个简单的数据，使得数据符合y = x1 + X2\n",
    "df = pd.DataFrame({'x1':[1,2,3,4,5,6],'x2':[1,2,1,2,1,2],'y':[2,4,4,6,6,8]})\n",
    "x = df[['x1','x2']]\n",
    "print(x)\n",
    "print(x.T)\n",
    "y = df['y'].values.reshape([-1,1])\n",
    "print(y)\n",
    "#BGD ：批量梯度下降------初始化theta,给定一个alpha\n",
    "theta = np.array([[0],[0]])\n",
    "print(theta)\n",
    "print(x.dot(theta))\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss为1.060800 \n",
      "theta为\n",
      " [[1.24]\n",
      " [0.48]]\n",
      "loss为0.825504 \n",
      "theta为\n",
      " [[1.1932]\n",
      " [0.4788]]\n",
      "loss为0.781794 \n",
      "theta为\n",
      " [[1.189384]\n",
      " [0.493224]]\n",
      "loss为0.740587 \n",
      "theta为\n",
      " [[1.18428064]\n",
      " [0.50674368]]\n",
      "loss为0.701552 \n",
      "theta为\n",
      " [[1.17935984]\n",
      " [0.51991952]]\n",
      "loss为0.664575 \n",
      "theta为\n",
      " [[1.17456895]\n",
      " [0.53274284]]\n",
      "loss为0.629546 \n",
      "theta为\n",
      " [[1.16990607]\n",
      " [0.54522366]]\n",
      "loss为0.596364 \n",
      "theta为\n",
      " [[1.16536774]\n",
      " [0.55737111]]\n",
      "loss为0.564931 \n",
      "theta为\n",
      " [[1.16095063]\n",
      " [0.56919409]]\n",
      "loss为0.535154 \n",
      "theta为\n",
      " [[1.15665151]\n",
      " [0.58070127]]\n",
      "loss为0.506947 \n",
      "theta为\n",
      " [[1.15246722]\n",
      " [0.59190108]]\n",
      "loss为0.480227 \n",
      "theta为\n",
      " [[1.14839469]\n",
      " [0.60280174]]\n",
      "loss为0.454915 \n",
      "theta为\n",
      " [[1.14443095]\n",
      " [0.61341123]]\n",
      "loss为0.430937 \n",
      "theta为\n",
      " [[1.14057308]\n",
      " [0.62373733]]\n",
      "loss为0.408223 \n",
      "theta为\n",
      " [[1.13681826]\n",
      " [0.63378762]]\n",
      "loss为0.386707 \n",
      "theta为\n",
      " [[1.13316373]\n",
      " [0.64356945]]\n",
      "loss为0.366324 \n",
      "theta为\n",
      " [[1.12960682]\n",
      " [0.65309   ]]\n",
      "loss为0.347016 \n",
      "theta为\n",
      " [[1.12614491]\n",
      " [0.66235625]]\n",
      "loss为0.328725 \n",
      "theta为\n",
      " [[1.12277548]\n",
      " [0.67137499]]\n",
      "loss为0.311399 \n",
      "theta为\n",
      " [[1.11949605]\n",
      " [0.68015283]]\n",
      "loss为0.294986 \n",
      "theta为\n",
      " [[1.11630421]\n",
      " [0.68869621]]\n",
      "loss为0.279437 \n",
      "theta为\n",
      " [[1.11319763]\n",
      " [0.69701139]]\n",
      "loss为0.264709 \n",
      "theta为\n",
      " [[1.11017403]\n",
      " [0.70510447]]\n",
      "loss为0.250756 \n",
      "theta为\n",
      " [[1.10723119]\n",
      " [0.71298137]]\n",
      "loss为0.237540 \n",
      "theta为\n",
      " [[1.10436696]\n",
      " [0.72064787]]\n",
      "loss为0.225019 \n",
      "theta为\n",
      " [[1.10157923]\n",
      " [0.72810959]]\n",
      "loss为0.213159 \n",
      "theta为\n",
      " [[1.09886596]\n",
      " [0.73537201]]\n",
      "loss为0.201924 \n",
      "theta为\n",
      " [[1.09622517]\n",
      " [0.74244044]]\n",
      "loss为0.191281 \n",
      "theta为\n",
      " [[1.09365492]\n",
      " [0.74932007]]\n",
      "loss为0.181199 \n",
      "theta为\n",
      " [[1.09115332]\n",
      " [0.75601593]]\n",
      "loss为0.171648 \n",
      "theta为\n",
      " [[1.08871854]\n",
      " [0.76253295]]\n",
      "loss为0.162601 \n",
      "theta为\n",
      " [[1.0863488 ]\n",
      " [0.76887589]]\n",
      "loss为0.154030 \n",
      "theta为\n",
      " [[1.08404235]\n",
      " [0.7750494 ]]\n",
      "loss为0.145912 \n",
      "theta为\n",
      " [[1.08179751]\n",
      " [0.78105802]]\n",
      "loss为0.138221 \n",
      "theta为\n",
      " [[1.07961263]\n",
      " [0.78690614]]\n",
      "loss为0.130936 \n",
      "theta为\n",
      " [[1.07748611]\n",
      " [0.79259805]]\n",
      "loss为0.124034 \n",
      "theta为\n",
      " [[1.07541639]\n",
      " [0.79813792]]\n",
      "loss为0.117497 \n",
      "theta为\n",
      " [[1.07340196]\n",
      " [0.80352982]]\n",
      "loss为0.111304 \n",
      "theta为\n",
      " [[1.07144133]\n",
      " [0.8087777 ]]\n",
      "loss为0.105437 \n",
      "theta为\n",
      " [[1.06953308]\n",
      " [0.81388541]]\n",
      "loss为0.099880 \n",
      "theta为\n",
      " [[1.06767579]\n",
      " [0.81885668]]\n",
      "loss为0.094615 \n",
      "theta为\n",
      " [[1.06586812]\n",
      " [0.82369517]]\n",
      "loss为0.089628 \n",
      "theta为\n",
      " [[1.06410873]\n",
      " [0.82840441]]\n",
      "loss为0.084904 \n",
      "theta为\n",
      " [[1.06239633]\n",
      " [0.83298787]]\n",
      "loss为0.080429 \n",
      "theta为\n",
      " [[1.06072967]\n",
      " [0.8374489 ]]\n",
      "loss为0.076190 \n",
      "theta为\n",
      " [[1.05910753]\n",
      " [0.84179078]]\n",
      "loss为0.072174 \n",
      "theta为\n",
      " [[1.05752872]\n",
      " [0.84601667]]\n",
      "loss为0.068370 \n",
      "theta为\n",
      " [[1.05599208]\n",
      " [0.85012969]]\n",
      "loss为0.064766 \n",
      "theta为\n",
      " [[1.05449649]\n",
      " [0.85413285]]\n",
      "loss为0.061352 \n",
      "theta为\n",
      " [[1.05304084]\n",
      " [0.85802908]]\n",
      "loss为0.058119 \n",
      "theta为\n",
      " [[1.05162408]\n",
      " [0.86182124]]\n",
      "loss为0.055055 \n",
      "theta为\n",
      " [[1.05024516]\n",
      " [0.86551211]]\n",
      "loss为0.052153 \n",
      "theta为\n",
      " [[1.04890307]\n",
      " [0.86910439]]\n",
      "loss为0.049404 \n",
      "theta为\n",
      " [[1.04759683]\n",
      " [0.87260072]]\n",
      "loss为0.046800 \n",
      "theta为\n",
      " [[1.04632548]\n",
      " [0.87600366]]\n",
      "loss为0.044334 \n",
      "theta为\n",
      " [[1.04508808]\n",
      " [0.8793157 ]]\n",
      "loss为0.041997 \n",
      "theta为\n",
      " [[1.04388375]\n",
      " [0.88253928]]\n",
      "loss为0.039783 \n",
      "theta为\n",
      " [[1.04271157]\n",
      " [0.88567675]]\n",
      "loss为0.037686 \n",
      "theta为\n",
      " [[1.04157071]\n",
      " [0.88873042]]\n",
      "loss为0.035700 \n",
      "theta为\n",
      " [[1.04046033]\n",
      " [0.89170252]]\n",
      "loss为0.033818 \n",
      "theta为\n",
      " [[1.0393796 ]\n",
      " [0.89459524]]\n",
      "loss为0.032036 \n",
      "theta为\n",
      " [[1.03832774]\n",
      " [0.89741068]]\n",
      "loss为0.030347 \n",
      "theta为\n",
      " [[1.03730397]\n",
      " [0.90015093]]\n",
      "loss为0.028748 \n",
      "theta为\n",
      " [[1.03630755]\n",
      " [0.90281798]]\n",
      "loss为0.027233 \n",
      "theta为\n",
      " [[1.03533775]\n",
      " [0.90541379]]\n",
      "loss为0.025797 \n",
      "theta为\n",
      " [[1.03439385]\n",
      " [0.90794026]]\n",
      "loss为0.024437 \n",
      "theta为\n",
      " [[1.03347516]\n",
      " [0.91039926]]\n",
      "loss为0.023149 \n",
      "theta为\n",
      " [[1.03258101]\n",
      " [0.91279257]]\n",
      "loss为0.021929 \n",
      "theta为\n",
      " [[1.03171074]\n",
      " [0.91512195]]\n",
      "loss为0.020773 \n",
      "theta为\n",
      " [[1.03086372]\n",
      " [0.91738911]]\n",
      "loss为0.019678 \n",
      "theta为\n",
      " [[1.03003933]\n",
      " [0.91959571]]\n",
      "loss为0.018641 \n",
      "theta为\n",
      " [[1.02923695]\n",
      " [0.92174338]]\n",
      "loss为0.017659 \n",
      "theta为\n",
      " [[1.02845601]\n",
      " [0.92383368]]\n",
      "loss为0.016728 \n",
      "theta为\n",
      " [[1.02769593]\n",
      " [0.92586814]]\n",
      "loss为0.015846 \n",
      "theta为\n",
      " [[1.02695615]\n",
      " [0.92784826]]\n",
      "loss为0.015011 \n",
      "theta为\n",
      " [[1.02623613]\n",
      " [0.9297755 ]]\n",
      "loss为0.014220 \n",
      "theta为\n",
      " [[1.02553534]\n",
      " [0.93165125]]\n",
      "loss为0.013470 \n",
      "theta为\n",
      " [[1.02485327]\n",
      " [0.9334769 ]]\n",
      "loss为0.012760 \n",
      "theta为\n",
      " [[1.02418942]\n",
      " [0.93525379]]\n",
      "loss为0.012088 \n",
      "theta为\n",
      " [[1.0235433 ]\n",
      " [0.93698321]]\n",
      "loss为0.011451 \n",
      "theta为\n",
      " [[1.02291444]\n",
      " [0.93866644]]\n",
      "loss为0.010847 \n",
      "theta为\n",
      " [[1.02230237]\n",
      " [0.94030471]]\n",
      "loss为0.010275 \n",
      "theta为\n",
      " [[1.02170666]\n",
      " [0.94189922]]\n",
      "loss为0.009734 \n",
      "theta为\n",
      " [[1.02112686]\n",
      " [0.94345114]]\n",
      "loss为0.009221 \n",
      "theta为\n",
      " [[1.02056254]\n",
      " [0.94496161]]\n",
      "loss为0.008735 \n",
      "theta为\n",
      " [[1.0200133 ]\n",
      " [0.94643173]]\n",
      "loss为0.008274 \n",
      "theta为\n",
      " [[1.01947873]\n",
      " [0.94786258]]\n",
      "loss为0.007838 \n",
      "theta为\n",
      " [[1.01895843]\n",
      " [0.94925521]]\n",
      "loss为0.007425 \n",
      "theta为\n",
      " [[1.01845204]\n",
      " [0.95061065]]\n",
      "loss为0.007034 \n",
      "theta为\n",
      " [[1.01795917]\n",
      " [0.95192988]]\n",
      "loss为0.006663 \n",
      "theta为\n",
      " [[1.01747947]\n",
      " [0.95321387]]\n",
      "loss为0.006312 \n",
      "theta为\n",
      " [[1.01701257]\n",
      " [0.95446357]]\n",
      "loss为0.005979 \n",
      "theta为\n",
      " [[1.01655815]\n",
      " [0.95567988]]\n",
      "loss为0.005664 \n",
      "theta为\n",
      " [[1.01611587]\n",
      " [0.95686371]]\n",
      "loss为0.005365 \n",
      "theta为\n",
      " [[1.0156854 ]\n",
      " [0.95801591]]\n",
      "loss为0.005083 \n",
      "theta为\n",
      " [[1.01526643]\n",
      " [0.95913734]]\n",
      "loss为0.004815 \n",
      "theta为\n",
      " [[1.01485866]\n",
      " [0.96022882]]\n",
      "loss为0.004561 \n",
      "theta为\n",
      " [[1.01446177]\n",
      " [0.96129114]]\n",
      "loss为0.004321 \n",
      "theta为\n",
      " [[1.01407548]\n",
      " [0.96232508]]\n",
      "loss为0.004093 \n",
      "theta为\n",
      " [[1.01369952]\n",
      " [0.96333141]]\n"
     ]
    }
   ],
   "source": [
    "#BGD 迭代\n",
    "for i in range(100):\n",
    "    theta = theta - alpha * x.T.dot(x.dot(theta)-y)\n",
    "    loss = np.power(x.dot(theta)-y,2).sum()\n",
    "    print('loss为%f \\ntheta为\\n'%loss,theta.values)\n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 随机梯度下降法（StaochasticGradient Descent,简称$SGD$）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "随机梯度下降法，其实和批量梯度下降法原理类似，区别在于求梯度是没有用所有的$m$个样本的数据，而是仅仅选取一个样本$j$来求梯度：\n",
    "\n",
    "\n",
    "For $i$ = 1 to m {\n",
    "$$     \n",
    "    \\theta_{j}: = \\theta_{j} -  \\alpha\\left(y^{(i)} -  h_{\\theta}(x^{(i)})\\right)x_{j}^{(i)}\n",
    "$$}\n",
    "\n",
    "随机梯度下降法批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。小批量梯度下降结合了两种算法的优点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]]\n",
      "[[6 2]\n",
      " [4 2]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [5 1]\n",
      " [1 1]]\n",
      "[4 2]\n",
      "loss值为：602.764800 \theta值为：\n",
      " [0.36 0.12]\n",
      "loss值为：381.746826 \theta值为：\n",
      " [0.6384 0.2592]\n",
      "loss值为：307.104594 \theta值为：\n",
      " [0.770688 0.391488]\n",
      "loss值为：273.270192 \theta值为：\n",
      " [1.06736832 0.49038144]\n",
      "loss值为：276.127306 \theta值为：\n",
      " [1.09328486 0.49556475]\n",
      "loss值为：335.893792 \theta值为：\n",
      " [1.28561938 0.68789926]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#定义一个简单的数据，使得数据符合y = x1 + X2\n",
    "df = pd.DataFrame({'x1':[1,2,3,4,5,6],'x2':[1,2,1,2,1,2],'y':[2,4,4,6,6,8]})\n",
    "x = df[['x1','x2']]\n",
    "y = df['y'].values.reshape([-1,1])\n",
    "#BGD ：批量梯度下降------初始化theta,给定一个alpha\n",
    "theta = np.array([[0],[0]])\n",
    "alpha = 0.01\n",
    "print(theta)\n",
    "#随机梯度下降-SGD\n",
    "#初始化theta,给定一个alpha\n",
    "x_s = x.sample(frac=1).values\n",
    "print(x_s)\n",
    "theta_s = np.array([0,0])\n",
    "alpha_s = 0.03\n",
    "print(x_s[1])\n",
    "for i in range(6):\n",
    "    theta_s = theta_s - alpha_s * (x_s[i].dot(theta_s) - y[i]) * x_s[i].T\n",
    "    loss = np.power(x_s.dot(theta_s) - y,2).sum()\n",
    "    print(\"loss值为：%f \\theta值为：\\n\" %loss,theta_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小批量梯度下降法（Mini-Batch Gradient Descent,简称$MBGD$）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小批量梯度下降法是批量梯度下降法和随机梯度下降法的折中，我们选择$m$个样本中的$x$样本来迭代，一般取$x=10$,视情况而定:\n",
    "    \n",
    "$$\n",
    "\\theta_{j}: = \\theta_{j} - \\alpha\\sum_{i=t}^{t+x-1}\\left(y^{(i)} -  h_{\\theta}(x^{(i)})\\right)x_{j}^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]\n",
      " [4]\n",
      " [4]\n",
      " [6]\n",
      " [6]\n",
      " [8]]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "loss值为：84.280000 \theta值为：\n",
      " [[0.3]\n",
      " [0.3]]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "loss值为：18.424000 \theta值为：\n",
      " [[0.72 ]\n",
      " [0.552]]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "loss值为：0.317356 \theta值为：\n",
      " [[1.07784]\n",
      " [0.7116 ]]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "loss值为：0.232787 \theta值为：\n",
      " [[1.0945728]\n",
      " [0.7245024]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#定义一个简单的数据，使得数据符合y = x1 + X2\n",
    "df = pd.DataFrame({'x1':[1,2,3,4,5,6],'x2':[1,2,1,2,1,2],'y':[2,4,4,6,6,8]})\n",
    "x = df[['x1','x2']]\n",
    "\n",
    "y = df['y'].values.reshape([-1,1])\n",
    "print(y)\n",
    "#小批量梯度下降\n",
    "theta_m = np.array([[0],[0]])\n",
    "alpha_m = 0.03\n",
    "flag = True\n",
    "for i in range(4):\n",
    "    theta_m = theta_m -alpha_m * x[i:i+2].T.dot(x[i:i+2].dot(theta_m)-y[i:i+2])\n",
    "    loss = np.power(x.dot(theta_m)-y,2).sum()\n",
    "    print(type(theta_m))\n",
    "    print(\"loss值为：%f \\theta值为：\\n\"%loss,theta_m.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 梯度下降案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchGradientDescent:\n",
      " [[6.85468903e+60]\n",
      " [1.07701436e+61]\n",
      " [1.46855982e+61]\n",
      " [1.75455326e+61]\n",
      " [1.88221869e+61]\n",
      " [2.11543613e+61]\n",
      " [2.61253360e+61]]\n",
      "Y_test:\n",
      "               0\n",
      "0  2.496815e+62\n",
      "1  3.685191e+62\n",
      "2  1.159578e+62\n",
      "StochasticGradientDescent:\n",
      " [[4.18648666e+27]\n",
      " [5.28081439e+27]\n",
      " [6.37514211e+27]\n",
      " [7.46191153e+27]\n",
      " [8.53734350e+27]\n",
      " [9.62033377e+27]\n",
      " [1.07222198e+28]]\n",
      "Y_test:\n",
      "               0\n",
      "0  1.102537e+29\n",
      "1  1.635504e+29\n",
      "2  5.218425e+28\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Jun 10 22:36:52 2018\n",
    "\n",
    "@author: hcc\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#批量梯度下降法\n",
    "def batchGradientDescent(train_X,train_Y,theta,alpha,m,maxIteration):\n",
    "    train_X = train_X.sample(frac=1).values\n",
    "    for i in range(0,maxIteration):\n",
    "        hypothesis =np.dot(train_X,theta)\n",
    "        loss = hypothesis - train_Y\n",
    "        #一行表示一个样本，转置后样本第i行由所有样本的第i个特征组成，\n",
    "        #np.dot(xTrain,loss)就是对所有样本的第i个特征求累加,得到第i个theta,\n",
    "#         print(loss)\n",
    "        gradient = train_X.T.dot(loss)\n",
    "        theta = theta - alpha * gradient\n",
    "        i+= 1\n",
    "    return theta\n",
    "# #随机梯度下降法\n",
    "def StochasticGradientDescent(train_X,train_Y,theta,alpha,m,maxIteration):\n",
    "    data = [i for i in range(m)]\n",
    "    #xTrains = train_X.transpose()\n",
    "    xTrains = train_X.sample(frac=1).values\n",
    "\n",
    "    for i in range(0,maxIteration):\n",
    "        index = random.sample(data,1)           #任意选取一个样本点，得到它的下标,便于下面找到xTrains的对应列\n",
    "        index1 = index[0]    \n",
    "        hypothesis =np.dot(xTrains[index1],theta)\n",
    "        loss = hypothesis - train_Y[index1]\n",
    "        gradient = np.mat(xTrains[index1]).T.dot(np.mat(loss))\n",
    "        theta = theta - alpha * gradient\n",
    "    return np.mat(theta)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    #定义一个简单的数据，使得数据符合y = x1 + X2\n",
    "    ##定义数据集，共3个样本，每个样本7个特征\n",
    "    train_Data= pd.DataFrame({'x1':[1.1,1.5,1],\n",
    "                       'x2':[1.3,1.9,2.1],\n",
    "                       'x3':[1.5,2.3,3.2],\n",
    "                       'x4':[1.7,2.7,3.9],\n",
    "                       'x5':[1.9,3.1,4],\n",
    "                       'x6':[2.1,3.5,4.5],\n",
    "                       'x7':[2.3,3.9,6]\n",
    "                      })\n",
    "    train_Label = pd.DataFrame({ 'y':[3.5,4.6,5.3]})\n",
    "    train_X = train_Data[['x1','x2','x3','x4','x5','x6','x7']]\n",
    "    train_Y = train_Label['y'].values.reshape([-1,1])\n",
    "    #     print(train_Y)\n",
    "    test_Data = pd.DataFrame({'x1':[1.2,1.5,1],\n",
    "                        'x2':[1.3,2.9,1],\n",
    "                        'x3':[1.5,3.3,1],\n",
    "                        'x4':[3.7,2.7,1],\n",
    "                        'x5':[1.8,3.1,1],\n",
    "                        'x6':[2.2,3.6,1],\n",
    "                        'x7':[2.3,3.7,1]\n",
    "                      })\n",
    "\n",
    "    m,n = np.shape(train_X)\n",
    "    theta = np.ones((n,1))\n",
    "    alpha = 0.1\n",
    "#    print(theta)\n",
    "    #设置迭代次数，也可以设置条件停止\n",
    "    maxIteration = 50\n",
    "    theta = batchGradientDescent(train_X, train_Y, theta, alpha, m, maxIteration)\n",
    "    print(\"batchGradientDescent:\\n\",theta.reshape(n,1))\n",
    "    print(\"Y_test:\\n\",test_Data.dot(theta.reshape(n,1)))\n",
    "    theta = np.ones((n,1))\n",
    "    alpha = 0.1\n",
    "    theta = StochasticGradientDescent(train_X, train_Y, theta, alpha, m, maxIteration)\n",
    "    print(\"StochasticGradientDescent:\\n\",theta.reshape(n,1))\n",
    "    print(\"Y_test:\\n\",test_Data.dot(theta.reshape(n,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
